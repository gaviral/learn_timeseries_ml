{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "!pip install yfinance pandas numpy scikit-learn matplotlib lightgbm xgboost catboost statsmodels tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Data Acquisition\n",
    "stock_symbol = 'SOXL'\n",
    "start_date = '2010-03-11'\n",
    "end_date = '2024-11-22'\n",
    "\n",
    "# Download stock data\n",
    "data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Check if data was downloaded successfully\n",
    "if data.empty:\n",
    "    raise ValueError(f\"No data found for stock symbol {stock_symbol} between {start_date} and {end_date}.\")\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "# We'll use the 'Close' price for prediction\n",
    "data = data[['Close']].copy()\n",
    "data.rename(columns={'Close': 'y'}, inplace=True)\n",
    "\n",
    "# Reset index to have 'Date' as a column\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "# Ensure data is sorted by date\n",
    "data.sort_values('Date', inplace=True)\n",
    "\n",
    "# Feature Engineering: Create additional time-based features\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "data['WeekOfYear'] = data['Date'].dt.isocalendar().week\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Quarter'] = data['Date'].dt.quarter\n",
    "data['Year'] = data['Date'].dt.year\n",
    "\n",
    "# Lag features (previous day's closing price)\n",
    "for lag in range(1, 8):  # Using 7 days lag\n",
    "    data[f'Lag_{lag}'] = data['y'].shift(lag)\n",
    "\n",
    "# Rolling window features\n",
    "data['RollingMean_7'] = data['y'].rolling(window=7).mean()\n",
    "data['RollingStd_7'] = data['y'].rolling(window=7).std()\n",
    "\n",
    "data['RollingMean_14'] = data['y'].rolling(window=14).mean()\n",
    "data['RollingStd_14'] = data['y'].rolling(window=14).std()\n",
    "\n",
    "# Drop rows with NaN values resulting from lag and rolling calculations\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 3. Train-Test Split\n",
    "# We'll use the last 20% of the data as the test set\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(data) * split_ratio)\n",
    "\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# 4. Feature Scaling\n",
    "# Features to scale\n",
    "features_to_scale = ['y'] + [f'Lag_{lag}' for lag in range(1, 8)] + \\\n",
    "                    ['RollingMean_7', 'RollingStd_7', 'RollingMean_14', 'RollingStd_14']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = train_data.copy()\n",
    "test_data_scaled = test_data.copy()\n",
    "\n",
    "train_data_scaled[features_to_scale] = scaler.fit_transform(train_data[features_to_scale])\n",
    "test_data_scaled[features_to_scale] = scaler.transform(test_data[features_to_scale])\n",
    "\n",
    "# 5. Prepare Training and Testing Data\n",
    "X_train = train_data_scaled.drop(columns=['Date', 'y'])\n",
    "y_train = train_data_scaled['y']\n",
    "\n",
    "X_test = test_data_scaled.drop(columns=['Date', 'y'])\n",
    "y_test = test_data_scaled['y']\n",
    "\n",
    "# 6. Model Training and Evaluation\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(true, predicted, model_name):\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    return rmse\n",
    "\n",
    "# Function to plot predictions\n",
    "def plot_predictions(true, predicted, model_name):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(true.reset_index(drop=True), label='Actual', color='blue')\n",
    "    plt.plot(predicted, label='Predicted', color='red', alpha=0.7)\n",
    "    plt.title(f'Actual vs Predicted Closing Prices ({model_name})')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Closing Price (Scaled)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "### **Model 1: LightGBM Regressor**\n",
    "\n",
    "# Initialize the model\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_lgbm = lgbm.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "rmse_lgbm = evaluate_model(y_test, y_pred_lgbm, 'LightGBM Regressor')\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(y_test, y_pred_lgbm, 'LightGBM Regressor')\n",
    "\n",
    "### **Model 2: ARIMA**\n",
    "\n",
    "# For ARIMA, we'll use the original data without scaling\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Ensure stationarity\n",
    "result = adfuller(train_data['y'])\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "\n",
    "if result[1] > 0.05:\n",
    "    # Differencing to make the series stationary\n",
    "    train_data['y_diff'] = train_data['y'].diff().dropna()\n",
    "else:\n",
    "    train_data['y_diff'] = train_data['y']\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_order = (5, 1, 0)  # This can be adjusted or determined using AIC/BIC criteria\n",
    "arima_model = ARIMA(train_data['y'], order=arima_order)\n",
    "arima_model_fit = arima_model.fit()\n",
    "\n",
    "# Forecast\n",
    "forecast_steps = len(test_data)\n",
    "arima_forecast = arima_model_fit.forecast(steps=forecast_steps)\n",
    "\n",
    "# Evaluate\n",
    "rmse_arima = evaluate_model(test_data['y'], arima_forecast, 'ARIMA')\n",
    "\n",
    "# Plot predictions\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_data['y'].reset_index(drop=True), label='Actual', color='blue')\n",
    "plt.plot(arima_forecast.values, label='Predicted', color='red', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Closing Prices (ARIMA)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### **Model 3: LSTM Neural Network**\n",
    "\n",
    "# Prepare data for LSTM\n",
    "def create_lstm_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 10\n",
    "\n",
    "# Combine train and test data for LSTM scaling\n",
    "combined_data = pd.concat([train_data_scaled, test_data_scaled], axis=0)\n",
    "X_lstm = combined_data.drop(columns=['Date', 'y'])\n",
    "y_lstm = combined_data['y']\n",
    "\n",
    "# Create datasets\n",
    "X_lstm, y_lstm = create_lstm_dataset(X_lstm, y_lstm, time_steps)\n",
    "\n",
    "# Split back into train and test\n",
    "X_train_lstm = X_lstm[:split_index - time_steps]\n",
    "y_train_lstm = y_lstm[:split_index - time_steps]\n",
    "\n",
    "X_test_lstm = X_lstm[split_index - time_steps:]\n",
    "y_test_lstm = y_lstm[split_index - time_steps:]\n",
    "\n",
    "# Build LSTM Model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(32, return_sequences=False))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(X_train_lstm, y_train_lstm, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Predict\n",
    "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Evaluate\n",
    "rmse_lstm = evaluate_model(y_test_lstm, y_pred_lstm.flatten(), 'LSTM')\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(y_test_lstm, y_pred_lstm.flatten(), 'LSTM')\n",
    "\n",
    "# 7. Model Comparison\n",
    "\n",
    "print(\"\\nModel RMSE Comparison:\")\n",
    "print(f\"LightGBM Regressor RMSE: {rmse_lgbm:.4f}\")\n",
    "print(f\"ARIMA RMSE: {rmse_arima:.4f}\")\n",
    "print(f\"LSTM RMSE: {rmse_lstm:.4f}\")\n",
    "\n",
    "# 8. Final Model Selection and Prediction\n",
    "\n",
    "# Assuming LightGBM performed the best based on RMSE\n",
    "# We can retrain LightGBM on the entire dataset and make future predictions if needed\n",
    "\n",
    "# Retrain on entire dataset\n",
    "full_data = data.copy()\n",
    "full_data[features_to_scale] = scaler.fit_transform(full_data[features_to_scale])\n",
    "\n",
    "X_full = full_data.drop(columns=['Date', 'y'])\n",
    "y_full = full_data['y']\n",
    "\n",
    "lgbm_final = LGBMRegressor(random_state=42)\n",
    "lgbm_final.fit(X_full, y_full)\n",
    "\n",
    "# Future Prediction (Next Day Closing Price)\n",
    "# Prepare features for the next day\n",
    "last_row = full_data.iloc[-1:].copy()\n",
    "\n",
    "# Increment the date by one day\n",
    "next_date = last_row['Date'] + timedelta(days=1)\n",
    "last_row['Date'] = next_date\n",
    "\n",
    "# Update time-based features\n",
    "last_row['DayOfWeek'] = last_row['Date'].dt.dayofweek\n",
    "last_row['WeekOfYear'] = last_row['Date'].dt.isocalendar().week\n",
    "last_row['Month'] = last_row['Date'].dt.month\n",
    "last_row['Quarter'] = last_row['Date'].dt.quarter\n",
    "last_row['Year'] = last_row['Date'].dt.year\n",
    "\n",
    "# Update lag features\n",
    "for lag in range(1, 8):\n",
    "    last_row[f'Lag_{lag}'] = full_data['y'].shift(lag).iloc[-1]\n",
    "\n",
    "# Update rolling features\n",
    "last_row['RollingMean_7'] = full_data['y'].rolling(window=7).mean().iloc[-1]\n",
    "last_row['RollingStd_7'] = full_data['y'].rolling(window=7).std().iloc[-1]\n",
    "last_row['RollingMean_14'] = full_data['y'].rolling(window=14).mean().iloc[-1]\n",
    "last_row['RollingStd_14'] = full_data['y'].rolling(window=14).std().iloc[-1]\n",
    "\n",
    "# Drop NaN values\n",
    "last_row.dropna(inplace=True)\n",
    "\n",
    "# Scale features\n",
    "last_row[features_to_scale] = scaler.transform(last_row[features_to_scale])\n",
    "\n",
    "# Prepare input features\n",
    "X_next = last_row.drop(columns=['Date', 'y'])\n",
    "\n",
    "# Predict the next day's closing price\n",
    "next_day_prediction_scaled = lgbm_final.predict(X_next)\n",
    "\n",
    "# Inverse transform the prediction\n",
    "next_day_prediction = scaler.inverse_transform([[0]* (len(features_to_scale)-1) + [next_day_prediction_scaled[0]]])[-1][0]\n",
    "\n",
    "print(f\"\\nPredicted Next Day Closing Price: {next_day_prediction:.2f}\")\n",
    "\n",
    "# 9. Plotting the Actual vs Predicted Prices for Test Data\n",
    "\n",
    "# Inverse transform the scaled prices\n",
    "y_test_actual = scaler.inverse_transform(np.concatenate((np.zeros((len(y_test), len(features_to_scale)-1)), y_test.values.reshape(-1, 1)), axis=1))[:, -1]\n",
    "y_pred_actual = scaler.inverse_transform(np.concatenate((np.zeros((len(y_pred_lgbm), len(features_to_scale)-1)), y_pred_lgbm.reshape(-1, 1)), axis=1))[:, -1]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_data['Date'].reset_index(drop=True), y_test_actual, label='Actual', color='blue')\n",
    "plt.plot(test_data['Date'].reset_index(drop=True), y_pred_actual, label='Predicted', color='red', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Closing Prices (LightGBM)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# Note: Uncomment the following line if running in a fresh environment\n",
    "# !pip install yfinance pandas numpy scikit-learn matplotlib lightgbm xgboost catboost statsmodels tensorflow\n",
    "\n",
    "# Import libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Data Acquisition\n",
    "stock_symbol = 'SOXL'\n",
    "start_date = '2010-03-11'\n",
    "end_date = '2024-11-22'\n",
    "\n",
    "# Download stock data\n",
    "data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Check if data was downloaded successfully\n",
    "if data.empty:\n",
    "    raise ValueError(f\"No data found for stock symbol {stock_symbol} between {start_date} and {end_date}.\")\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "# We'll use the 'Close' price for prediction\n",
    "data = data[['Close']].copy()\n",
    "data.rename(columns={'Close': 'y'}, inplace=True)\n",
    "\n",
    "# Reset index to have 'Date' as a column\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "# Ensure data is sorted by date\n",
    "data.sort_values('Date', inplace=True)\n",
    "\n",
    "# Feature Engineering: Create additional time-based features\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "data['WeekOfYear'] = data['Date'].dt.isocalendar().week\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Quarter'] = data['Date'].dt.quarter\n",
    "data['Year'] = data['Date'].dt.year\n",
    "\n",
    "# Lag features (previous day's closing price)\n",
    "for lag in range(1, 8):  # Using 7 days lag\n",
    "    data[f'Lag_{lag}'] = data['y'].shift(lag)\n",
    "\n",
    "# Rolling window features\n",
    "data['RollingMean_7'] = data['y'].rolling(window=7).mean()\n",
    "data['RollingStd_7'] = data['y'].rolling(window=7).std()\n",
    "\n",
    "data['RollingMean_14'] = data['y'].rolling(window=14).mean()\n",
    "data['RollingStd_14'] = data['y'].rolling(window=14).std()\n",
    "\n",
    "# Drop rows with NaN values resulting from lag and rolling calculations\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 3. Train-Test Split\n",
    "# We'll use the last 20% of the data as the test set\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(data) * split_ratio)\n",
    "\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# 4. Feature Scaling\n",
    "# Features to scale\n",
    "features_to_scale = ['y'] + [f'Lag_{lag}' for lag in range(1, 8)] + \\\n",
    "                    ['RollingMean_7', 'RollingStd_7', 'RollingMean_14', 'RollingStd_14']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = train_data.copy()\n",
    "test_data_scaled = test_data.copy()\n",
    "\n",
    "train_data_scaled[features_to_scale] = scaler.fit_transform(train_data[features_to_scale])\n",
    "test_data_scaled[features_to_scale] = scaler.transform(test_data[features_to_scale])\n",
    "\n",
    "# 5. Prepare Training and Testing Data\n",
    "X_train = train_data_scaled.drop(columns=['Date', 'y'])\n",
    "y_train = train_data_scaled['y']\n",
    "\n",
    "X_test = test_data_scaled.drop(columns=['Date', 'y'])\n",
    "y_test = test_data_scaled['y']\n",
    "\n",
    "# 6. Sanitize Feature Names to Remove Special Characters\n",
    "def sanitize_column_names(columns):\n",
    "    \"\"\"\n",
    "    Replace any special characters in column names with underscores.\n",
    "    Ensures all column names are strings before applying regex.\n",
    "    \"\"\"\n",
    "    return [re.sub(r'[^A-Za-z0-9_]', '_', str(col)) for col in columns]\n",
    "\n",
    "# Apply sanitization\n",
    "X_train.columns = sanitize_column_names(X_train.columns)\n",
    "X_test.columns = sanitize_column_names(X_test.columns)\n",
    "\n",
    "# 7. Model Training and Evaluation\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(true, predicted, model_name):\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    return rmse\n",
    "\n",
    "# Function to plot predictions\n",
    "def plot_predictions(true, predicted, model_name):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(range(len(true)), true.values, label='Actual', color='blue')\n",
    "    plt.plot(range(len(predicted)), predicted, label='Predicted', color='red', alpha=0.7)\n",
    "    plt.title(f'Actual vs Predicted Closing Prices ({model_name})')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Closing Price (Scaled)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "### **Model 1: LightGBM Regressor**\n",
    "\n",
    "# Initialize the model\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    print(\"LightGBM training completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LightGBM training: {e}\")\n",
    "\n",
    "# Predict\n",
    "try:\n",
    "    y_pred_lgbm = lgbm.predict(X_test)\n",
    "    print(\"LightGBM prediction completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LightGBM prediction: {e}\")\n",
    "\n",
    "# Evaluate\n",
    "if 'y_pred_lgbm' in locals():\n",
    "    rmse_lgbm = evaluate_model(y_test, y_pred_lgbm, 'LightGBM Regressor')\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_predictions(y_test, y_pred_lgbm, 'LightGBM Regressor')\n",
    "else:\n",
    "    rmse_lgbm = None\n",
    "    print(\"Skipping evaluation and plotting for LightGBM due to previous errors.\")\n",
    "\n",
    "### **Model 2: ARIMA**\n",
    "\n",
    "# For ARIMA, we'll use the original data without scaling\n",
    "# Ensure stationarity\n",
    "result = adfuller(train_data['y'])\n",
    "print('\\nAugmented Dickey-Fuller Test:')\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "\n",
    "if result[1] > 0.05:\n",
    "    # Differencing to make the series stationary\n",
    "    train_data['y_diff'] = train_data['y'].diff().dropna()\n",
    "    print(\"Applied differencing to make the series stationary.\")\n",
    "else:\n",
    "    train_data['y_diff'] = train_data['y']\n",
    "    print(\"Series is stationary. No differencing applied.\")\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_order = (5, 1, 0)  # This can be adjusted or determined using AIC/BIC criteria\n",
    "arima_model = ARIMA(train_data['y'], order=arima_order)\n",
    "try:\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    print(\"ARIMA model fitted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during ARIMA model fitting: {e}\")\n",
    "\n",
    "# Forecast\n",
    "forecast_steps = len(test_data)\n",
    "try:\n",
    "    arima_forecast = arima_model_fit.forecast(steps=forecast_steps)\n",
    "    print(\"ARIMA forecasting completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during ARIMA forecasting: {e}\")\n",
    "\n",
    "# Evaluate\n",
    "try:\n",
    "    rmse_arima = evaluate_model(test_data['y'], arima_forecast, 'ARIMA')\n",
    "except Exception as e:\n",
    "    print(f\"Error during ARIMA evaluation: {e}\")\n",
    "    rmse_arima = None\n",
    "\n",
    "# Plot predictions\n",
    "if rmse_arima is not None:\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_data['Date'].reset_index(drop=True), test_data['y'].reset_index(drop=True), label='Actual', color='blue')\n",
    "    plt.plot(test_data['Date'].reset_index(drop=True), arima_forecast.values, label='Predicted', color='red', alpha=0.7)\n",
    "    plt.title('Actual vs Predicted Closing Prices (ARIMA)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Closing Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping ARIMA plotting due to previous errors.\")\n",
    "\n",
    "### **Model 3: LSTM Neural Network**\n",
    "\n",
    "# Prepare data for LSTM\n",
    "def create_lstm_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 10\n",
    "\n",
    "# Combine train and test data for LSTM scaling\n",
    "combined_data = pd.concat([train_data_scaled, test_data_scaled], axis=0)\n",
    "X_lstm = combined_data.drop(columns=['Date', 'y'])\n",
    "y_lstm = combined_data['y']\n",
    "\n",
    "# Create datasets\n",
    "X_lstm, y_lstm = create_lstm_dataset(X_lstm, y_lstm, time_steps)\n",
    "\n",
    "# Split back into train and test\n",
    "X_train_lstm = X_lstm[:split_index - time_steps]\n",
    "y_train_lstm = y_lstm[:split_index - time_steps]\n",
    "\n",
    "X_test_lstm = X_lstm[split_index - time_steps:]\n",
    "y_test_lstm = y_lstm[split_index - time_steps:]\n",
    "\n",
    "# Build LSTM Model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(32, return_sequences=False))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(X_train_lstm, y_train_lstm, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Predict\n",
    "y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Evaluate\n",
    "rmse_lstm = evaluate_model(y_test_lstm, y_pred_lstm.flatten(), 'LSTM')\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(y_test_lstm, y_pred_lstm.flatten(), 'LSTM')\n",
    "\n",
    "# 7. Model Comparison\n",
    "\n",
    "print(\"\\nModel RMSE Comparison:\")\n",
    "if rmse_lgbm is not None:\n",
    "    print(f\"LightGBM Regressor RMSE: {rmse_lgbm:.4f}\")\n",
    "if rmse_arima is not None:\n",
    "    print(f\"ARIMA RMSE: {rmse_arima:.4f}\")\n",
    "if rmse_lstm is not None:\n",
    "    print(f\"LSTM RMSE: {rmse_lstm:.4f}\")\n",
    "\n",
    "# 8. Final Model Selection and Prediction\n",
    "\n",
    "# Assuming LightGBM performed the best based on RMSE\n",
    "# We can retrain LightGBM on the entire dataset and make future predictions if needed\n",
    "\n",
    "if rmse_lgbm is not None and (rmse_arima is None or rmse_lgbm <= rmse_arima) and (rmse_lstm is None or rmse_lgbm <= rmse_lstm):\n",
    "    print(\"\\nSelecting LightGBM Regressor as the final model.\")\n",
    "    # Retrain on entire dataset\n",
    "    full_data = data.copy()\n",
    "    full_data[features_to_scale] = scaler.fit_transform(full_data[features_to_scale])\n",
    "    \n",
    "    X_full = full_data.drop(columns=['Date', 'y'])\n",
    "    y_full = full_data['y']\n",
    "    \n",
    "    # Sanitize column names in the full dataset\n",
    "    X_full.columns = sanitize_column_names(X_full.columns)\n",
    "    \n",
    "    lgbm_final = LGBMRegressor(random_state=42)\n",
    "    try:\n",
    "        lgbm_final.fit(X_full, y_full)\n",
    "        print(\"Final LightGBM training completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during final LightGBM training: {e}\")\n",
    "    \n",
    "    # Future Prediction (Next Day Closing Price)\n",
    "    # Prepare features for the next day\n",
    "    last_row = full_data.iloc[-1:].copy()\n",
    "    \n",
    "    # Increment the date by one day\n",
    "    next_date = last_row['Date'] + timedelta(days=1)\n",
    "    last_row['Date'] = next_date\n",
    "    \n",
    "    # Update time-based features\n",
    "    last_row['DayOfWeek'] = last_row['Date'].dt.dayofweek\n",
    "    last_row['WeekOfYear'] = last_row['Date'].dt.isocalendar().week\n",
    "    last_row['Month'] = last_row['Date'].dt.month\n",
    "    last_row['Quarter'] = last_row['Date'].dt.quarter\n",
    "    last_row['Year'] = last_row['Date'].dt.year\n",
    "    \n",
    "    # Update lag features\n",
    "    for lag in range(1, 8):\n",
    "        # Since we're predicting the next day, shift the 'y' values accordingly\n",
    "        if lag <= len(full_data):\n",
    "            last_row[f'Lag_{lag}'] = full_data['y'].shift(lag).iloc[-1]\n",
    "        else:\n",
    "            last_row[f'Lag_{lag}'] = 0  # Assign 0 or some default value if lag exceeds data length\n",
    "    \n",
    "    # Update rolling features\n",
    "    last_row['RollingMean_7'] = full_data['y'].rolling(window=7).mean().iloc[-1]\n",
    "    last_row['RollingStd_7'] = full_data['y'].rolling(window=7).std().iloc[-1]\n",
    "    last_row['RollingMean_14'] = full_data['y'].rolling(window=14).mean().iloc[-1]\n",
    "    last_row['RollingStd_14'] = full_data['y'].rolling(window=14).std().iloc[-1]\n",
    "    \n",
    "    # Drop NaN values\n",
    "    last_row.dropna(inplace=True)\n",
    "    \n",
    "    # Scale features\n",
    "    last_row[features_to_scale] = scaler.transform(last_row[features_to_scale])\n",
    "    \n",
    "    # Sanitize column names\n",
    "    last_row = last_row.rename(columns=lambda x: re.sub(r'[^A-Za-z0-9_]', '_', x))\n",
    "    \n",
    "    # Prepare input features\n",
    "    X_next = last_row.drop(columns=['Date', 'y'])\n",
    "    \n",
    "    # Predict the next day's closing price\n",
    "    try:\n",
    "        next_day_prediction_scaled = lgbm_final.predict(X_next)\n",
    "        print(\"Next day prediction completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LightGBM prediction for next day: {e}\")\n",
    "    \n",
    "    # Inverse transform the prediction\n",
    "    # Since only 'y' was scaled, we need to inverse transform only that component\n",
    "    # Create an array with zeros for other features\n",
    "    inverse_transform_input = np.zeros((1, len(features_to_scale)))\n",
    "    inverse_transform_input[0, -1] = next_day_prediction_scaled[0]\n",
    "    next_day_prediction = scaler.inverse_transform(inverse_transform_input)[0, -1]\n",
    "    \n",
    "    print(f\"\\nPredicted Next Day Closing Price: {next_day_prediction:.2f}\")\n",
    "else:\n",
    "    print(\"\\nLightGBM Regressor did not perform the best. Skipping final model selection and prediction.\")\n",
    "\n",
    "# 9. Plotting the Actual vs Predicted Prices for Test Data\n",
    "\n",
    "# Inverse transform the scaled prices\n",
    "y_test_actual = scaler.inverse_transform(\n",
    "    np.concatenate((np.zeros((len(y_test), len(features_to_scale)-1)), y_test.values.reshape(-1, 1)), axis=1)\n",
    ")[:, -1]\n",
    "y_pred_actual = scaler.inverse_transform(\n",
    "    np.concatenate((np.zeros((len(y_pred_lgbm), len(features_to_scale)-1)), y_pred_lgbm.reshape(-1, 1)), axis=1)\n",
    ")[:, -1]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_data['Date'].reset_index(drop=True), y_test_actual, label='Actual', color='blue')\n",
    "plt.plot(test_data['Date'].reset_index(drop=True), y_pred_actual, label='Predicted', color='red', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Closing Prices (LightGBM)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 10. Feature Importance Visualization\n",
    "\n",
    "if rmse_lgbm is not None:\n",
    "    importances = lgbm.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    # Create a DataFrame for visualization\n",
    "    feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(feature_importances['Feature'], feature_importances['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances from LightGBM Regressor')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# Note: Uncomment the following line if running in a fresh environment\n",
    "# !pip install yfinance pandas numpy scikit-learn matplotlib lightgbm xgboost catboost statsmodels tensorflow\n",
    "\n",
    "# Import libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Data Acquisition\n",
    "stock_symbol = 'SOXL'\n",
    "start_date = '2010-03-11'\n",
    "end_date = '2024-11-22'\n",
    "\n",
    "# Download stock data\n",
    "data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Check if data was downloaded successfully\n",
    "if data.empty:\n",
    "    raise ValueError(f\"No data found for stock symbol {stock_symbol} between {start_date} and {end_date}.\")\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "# We'll use the 'Close' price for prediction\n",
    "data = data[['Close']].copy()\n",
    "data.rename(columns={'Close': 'y'}, inplace=True)\n",
    "\n",
    "# Reset index to have 'Date' as a column\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "# Ensure data is sorted by date\n",
    "data.sort_values('Date', inplace=True)\n",
    "\n",
    "# Feature Engineering: Create additional time-based features\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "data['WeekOfYear'] = data['Date'].dt.isocalendar().week\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Quarter'] = data['Date'].dt.quarter\n",
    "data['Year'] = data['Date'].dt.year\n",
    "\n",
    "# Lag features (previous day's closing price)\n",
    "for lag in range(1, 8):  # Using 7 days lag\n",
    "    data[f'Lag_{lag}'] = data['y'].shift(lag)\n",
    "\n",
    "# Rolling window features\n",
    "data['RollingMean_7'] = data['y'].rolling(window=7).mean()\n",
    "data['RollingStd_7'] = data['y'].rolling(window=7).std()\n",
    "\n",
    "data['RollingMean_14'] = data['y'].rolling(window=14).mean()\n",
    "data['RollingStd_14'] = data['y'].rolling(window=14).std()\n",
    "\n",
    "# Drop rows with NaN values resulting from lag and rolling calculations\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 3. Train-Test Split\n",
    "# We'll use the last 20% of the data as the test set\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(data) * split_ratio)\n",
    "\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# 4. Feature Scaling\n",
    "# Features to scale\n",
    "features_to_scale = ['y'] + [f'Lag_{lag}' for lag in range(1, 8)] + \\\n",
    "                    ['RollingMean_7', 'RollingStd_7', 'RollingMean_14', 'RollingStd_14']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = train_data.copy()\n",
    "test_data_scaled = test_data.copy()\n",
    "\n",
    "train_data_scaled[features_to_scale] = scaler.fit_transform(train_data[features_to_scale])\n",
    "test_data_scaled[features_to_scale] = scaler.transform(test_data[features_to_scale])\n",
    "\n",
    "# 5. Prepare Training and Testing Data\n",
    "X_train = train_data_scaled.drop(columns=['Date', 'y'])\n",
    "y_train = train_data_scaled['y']\n",
    "\n",
    "X_test = test_data_scaled.drop(columns=['Date', 'y'])\n",
    "y_test = test_data_scaled['y']\n",
    "\n",
    "# 6. Sanitize Feature Names to Remove Special Characters\n",
    "def sanitize_column_names(columns):\n",
    "    \"\"\"\n",
    "    Replace any special characters in column names with underscores.\n",
    "    Ensures all column names are strings before applying regex.\n",
    "    \"\"\"\n",
    "    return [re.sub(r'[^A-Za-z0-9_]', '_', str(col)) for col in columns]\n",
    "\n",
    "# Apply sanitization\n",
    "X_train.columns = sanitize_column_names(X_train.columns)\n",
    "X_test.columns = sanitize_column_names(X_test.columns)\n",
    "\n",
    "# 7. Model Training and Evaluation\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(true, predicted, model_name):\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    return rmse\n",
    "\n",
    "# Function to plot predictions\n",
    "def plot_predictions(true, predicted, model_name):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(range(len(true)), true.values, label='Actual', color='blue')\n",
    "    plt.plot(range(len(predicted)), predicted, label='Predicted', color='red', alpha=0.7)\n",
    "    plt.title(f'Actual vs Predicted Closing Prices ({model_name})')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Closing Price (Scaled)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "### **Model 1: LightGBM Regressor**\n",
    "\n",
    "# Initialize the model\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    print(\"LightGBM training completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LightGBM training: {e}\")\n",
    "\n",
    "# Predict\n",
    "try:\n",
    "    y_pred_lgbm = lgbm.predict(X_test)\n",
    "    print(\"LightGBM prediction completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LightGBM prediction: {e}\")\n",
    "\n",
    "# Evaluate\n",
    "if 'y_pred_lgbm' in locals():\n",
    "    rmse_lgbm = evaluate_model(y_test, y_pred_lgbm, 'LightGBM Regressor')\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_predictions(y_test, y_pred_lgbm, 'LightGBM Regressor')\n",
    "else:\n",
    "    rmse_lgbm = None\n",
    "    print(\"Skipping evaluation and plotting for LightGBM due to previous errors.\")\n",
    "\n",
    "### **Model 2: ARIMA**\n",
    "\n",
    "# For ARIMA, we'll use the original data without scaling\n",
    "# Ensure stationarity\n",
    "result = adfuller(train_data['y'])\n",
    "print('\\nAugmented Dickey-Fuller Test:')\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "\n",
    "if result[1] > 0.05:\n",
    "    # Differencing to make the series stationary\n",
    "    train_data['y_diff'] = train_data['y'].diff().dropna()\n",
    "    print(\"Applied differencing to make the series stationary.\")\n",
    "else:\n",
    "    train_data['y_diff'] = train_data['y']\n",
    "    print(\"Series is stationary. No differencing applied.\")\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_order = (5, 1, 0)  # This can be adjusted or determined using AIC/BIC criteria\n",
    "arima_model = ARIMA(train_data['y'], order=arima_order)\n",
    "try:\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    print(\"ARIMA model fitted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during ARIMA model fitting: {e}\")\n",
    "\n",
    "# Forecast\n",
    "forecast_steps = len(test_data)\n",
    "try:\n",
    "    arima_forecast = arima_model_fit.forecast(steps=forecast_steps)\n",
    "    print(\"ARIMA forecasting completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during ARIMA forecasting: {e}\")\n",
    "\n",
    "# Evaluate\n",
    "try:\n",
    "    rmse_arima = evaluate_model(test_data['y'], arima_forecast, 'ARIMA')\n",
    "except Exception as e:\n",
    "    print(f\"Error during ARIMA evaluation: {e}\")\n",
    "    rmse_arima = None\n",
    "\n",
    "# Plot predictions\n",
    "if rmse_arima is not None:\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_data['Date'].reset_index(drop=True), test_data['y'].reset_index(drop=True), label='Actual', color='blue')\n",
    "    plt.plot(test_data['Date'].reset_index(drop=True), arima_forecast.values, label='Predicted', color='red', alpha=0.7)\n",
    "    plt.title('Actual vs Predicted Closing Prices (ARIMA)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Closing Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping ARIMA plotting due to previous errors.\")\n",
    "\n",
    "### **Model 3: LSTM Neural Network**\n",
    "\n",
    "# Prepare data for LSTM\n",
    "def create_lstm_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.float32)\n",
    "\n",
    "time_steps = 10\n",
    "\n",
    "# Combine train and test data for LSTM scaling\n",
    "combined_data = pd.concat([train_data_scaled, test_data_scaled], axis=0)\n",
    "X_lstm = combined_data.drop(columns=['Date', 'y'])\n",
    "y_lstm = combined_data['y']\n",
    "\n",
    "# Create datasets\n",
    "X_lstm, y_lstm = create_lstm_dataset(X_lstm, y_lstm, time_steps)\n",
    "\n",
    "# Split back into train and test\n",
    "X_train_lstm = X_lstm[:split_index - time_steps]\n",
    "y_train_lstm = y_lstm[:split_index - time_steps]\n",
    "\n",
    "X_test_lstm = X_lstm[split_index - time_steps:]\n",
    "y_test_lstm = y_lstm[split_index - time_steps:]\n",
    "\n",
    "# Build LSTM Model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(32, return_sequences=False))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    history = lstm_model.fit(X_train_lstm, y_train_lstm, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "    print(\"LSTM training completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LSTM training: {e}\")\n",
    "\n",
    "# Predict\n",
    "try:\n",
    "    y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
    "    print(\"LSTM prediction completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LSTM prediction: {e}\")\n",
    "\n",
    "# Evaluate\n",
    "if 'y_pred_lstm' in locals():\n",
    "    rmse_lstm = evaluate_model(y_test_lstm, y_pred_lstm.flatten(), 'LSTM')\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_predictions(y_test_lstm, y_pred_lstm.flatten(), 'LSTM')\n",
    "else:\n",
    "    rmse_lstm = None\n",
    "    print(\"Skipping evaluation and plotting for LSTM due to previous errors.\")\n",
    "\n",
    "# 7. Model Comparison\n",
    "\n",
    "print(\"\\nModel RMSE Comparison:\")\n",
    "if rmse_lgbm is not None:\n",
    "    print(f\"LightGBM Regressor RMSE: {rmse_lgbm:.4f}\")\n",
    "if rmse_arima is not None:\n",
    "    print(f\"ARIMA RMSE: {rmse_arima:.4f}\")\n",
    "if rmse_lstm is not None:\n",
    "    print(f\"LSTM RMSE: {rmse_lstm:.4f}\")\n",
    "\n",
    "# 8. Final Model Selection and Prediction\n",
    "\n",
    "# Assuming LightGBM performed the best based on RMSE\n",
    "if rmse_lgbm is not None and (rmse_arima is None or rmse_lgbm <= rmse_arima) and (rmse_lstm is None or rmse_lgbm <= rmse_lstm):\n",
    "    print(\"\\nSelecting LightGBM Regressor as the final model.\")\n",
    "    # Retrain on entire dataset\n",
    "    full_data = data.copy()\n",
    "    full_data[features_to_scale] = scaler.fit_transform(full_data[features_to_scale])\n",
    "    \n",
    "    X_full = full_data.drop(columns=['Date', 'y'])\n",
    "    y_full = full_data['y']\n",
    "    \n",
    "    # Sanitize column names in the full dataset\n",
    "    X_full.columns = sanitize_column_names(X_full.columns)\n",
    "    \n",
    "    lgbm_final = LGBMRegressor(random_state=42)\n",
    "    try:\n",
    "        lgbm_final.fit(X_full, y_full)\n",
    "        print(\"Final LightGBM training completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during final LightGBM training: {e}\")\n",
    "    \n",
    "    # Future Prediction (Next Day Closing Price)\n",
    "    # Prepare features for the next day\n",
    "    last_row = full_data.iloc[-1:].copy()\n",
    "    \n",
    "    # Increment the date by one day\n",
    "    next_date = last_row['Date'] + timedelta(days=1)\n",
    "    last_row['Date'] = next_date\n",
    "    \n",
    "    # Update time-based features\n",
    "    last_row['DayOfWeek'] = last_row['Date'].dt.dayofweek\n",
    "    last_row['WeekOfYear'] = last_row['Date'].dt.isocalendar().week\n",
    "    last_row['Month'] = last_row['Date'].dt.month\n",
    "    last_row['Quarter'] = last_row['Date'].dt.quarter\n",
    "    last_row['Year'] = last_row['Date'].dt.year\n",
    "    \n",
    "    # Update lag features\n",
    "    for lag in range(1, 8):\n",
    "        if lag <= len(full_data):\n",
    "            last_row[f'Lag_{lag}'] = full_data['y'].shift(lag).iloc[-1]\n",
    "        else:\n",
    "            last_row[f'Lag_{lag}'] = 0  # Assign 0 or some default value if lag exceeds data length\n",
    "    \n",
    "    # Update rolling features\n",
    "    last_row['RollingMean_7'] = full_data['y'].rolling(window=7).mean().iloc[-1]\n",
    "    last_row['RollingStd_7'] = full_data['y'].rolling(window=7).std().iloc[-1]\n",
    "    last_row['RollingMean_14'] = full_data['y'].rolling(window=14).mean().iloc[-1]\n",
    "    last_row['RollingStd_14'] = full_data['y'].rolling(window=14).std().iloc[-1]\n",
    "    \n",
    "    # Drop NaN values\n",
    "    last_row.dropna(inplace=True)\n",
    "    \n",
    "    # Scale features\n",
    "    last_row[features_to_scale] = scaler.transform(last_row[features_to_scale])\n",
    "    \n",
    "    # Sanitize column names\n",
    "    last_row = last_row.rename(columns=lambda x: re.sub(r'[^A-Za-z0-9_]', '_', x))\n",
    "    \n",
    "    # Prepare input features\n",
    "    X_next = last_row.drop(columns=['Date', 'y'])\n",
    "    \n",
    "    # Predict the next day's closing price\n",
    "    try:\n",
    "        next_day_prediction_scaled = lgbm_final.predict(X_next)\n",
    "        print(\"Next day prediction completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LightGBM prediction for next day: {e}\")\n",
    "    \n",
    "    # Inverse transform the prediction\n",
    "    # Since only 'y' was scaled, we need to inverse transform only that component\n",
    "    # Create an array with zeros for other features\n",
    "    inverse_transform_input = np.zeros((1, len(features_to_scale)))\n",
    "    inverse_transform_input[0, -1] = next_day_prediction_scaled[0]\n",
    "    next_day_prediction = scaler.inverse_transform(inverse_transform_input)[0, -1]\n",
    "    \n",
    "    print(f\"\\nPredicted Next Day Closing Price: {next_day_prediction:.2f}\")\n",
    "else:\n",
    "    print(\"\\nLightGBM Regressor did not perform the best. Skipping final model selection and prediction.\")\n",
    "\n",
    "# 9. Plotting the Actual vs Predicted Prices for Test Data\n",
    "\n",
    "# Inverse transform the scaled prices\n",
    "y_test_actual = scaler.inverse_transform(\n",
    "    np.concatenate((np.zeros((len(y_test), len(features_to_scale)-1)), y_test.values.reshape(-1, 1)), axis=1)\n",
    ")[:, -1]\n",
    "y_pred_actual = scaler.inverse_transform(\n",
    "    np.concatenate((np.zeros((len(y_pred_lgbm), len(features_to_scale)-1)), y_pred_lgbm.reshape(-1, 1)), axis=1)\n",
    ")[:, -1]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_data['Date'].reset_index(drop=True), y_test_actual, label='Actual', color='blue')\n",
    "plt.plot(test_data['Date'].reset_index(drop=True), y_pred_actual, label='Predicted', color='red', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Closing Prices (LightGBM)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 10. Feature Importance Visualization\n",
    "\n",
    "if rmse_lgbm is not None:\n",
    "    importances = lgbm.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    # Create a DataFrame for visualization\n",
    "    feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(feature_importances['Feature'], feature_importances['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances from LightGBM Regressor')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# Note: Uncomment the following line if running in a fresh environment\n",
    "# !pip install yfinance pandas numpy scikit-learn matplotlib lightgbm xgboost catboost statsmodels tensorflow\n",
    "\n",
    "# Import libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Data Acquisition\n",
    "stock_symbol = 'SOXL'\n",
    "start_date = '2010-03-11'\n",
    "end_date = '2024-11-22'\n",
    "\n",
    "# Download stock data\n",
    "data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Check if data was downloaded successfully\n",
    "if data.empty:\n",
    "    raise ValueError(f\"No data found for stock symbol {stock_symbol} between {start_date} and {end_date}.\")\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "# We'll use the 'Close' price for prediction\n",
    "data = data[['Close']].copy()\n",
    "data.rename(columns={'Close': 'y'}, inplace=True)\n",
    "\n",
    "# Reset index to have 'Date' as a column\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "# Ensure data is sorted by date\n",
    "data.sort_values('Date', inplace=True)\n",
    "\n",
    "# Feature Engineering: Create additional time-based features\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "data['WeekOfYear'] = data['Date'].dt.isocalendar().week\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Quarter'] = data['Date'].dt.quarter\n",
    "data['Year'] = data['Date'].dt.year\n",
    "\n",
    "# Lag features (previous day's closing price)\n",
    "for lag in range(1, 8):  # Using 7 days lag\n",
    "    data[f'Lag_{lag}'] = data['y'].shift(lag)\n",
    "\n",
    "# Rolling window features\n",
    "data['RollingMean_7'] = data['y'].rolling(window=7).mean()\n",
    "data['RollingStd_7'] = data['y'].rolling(window=7).std()\n",
    "\n",
    "data['RollingMean_14'] = data['y'].rolling(window=14).mean()\n",
    "data['RollingStd_14'] = data['y'].rolling(window=14).std()\n",
    "\n",
    "# Drop rows with NaN values resulting from lag and rolling calculations\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 3. Train-Test Split\n",
    "# We'll use the last 20% of the data as the test set\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(data) * split_ratio)\n",
    "\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# 4. Feature Scaling\n",
    "# Features to scale\n",
    "features_to_scale = ['y'] + [f'Lag_{lag}' for lag in range(1, 8)] + \\\n",
    "                    ['RollingMean_7', 'RollingStd_7', 'RollingMean_14', 'RollingStd_14']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_scaled = train_data.copy()\n",
    "test_data_scaled = test_data.copy()\n",
    "\n",
    "train_data_scaled[features_to_scale] = scaler.fit_transform(train_data[features_to_scale])\n",
    "test_data_scaled[features_to_scale] = scaler.transform(test_data[features_to_scale])\n",
    "\n",
    "# 5. Prepare Training and Testing Data\n",
    "X_train = train_data_scaled.drop(columns=['Date', 'y'])\n",
    "y_train = train_data_scaled['y']\n",
    "\n",
    "X_test = test_data_scaled.drop(columns=['Date', 'y'])\n",
    "y_test = test_data_scaled['y']\n",
    "\n",
    "# 6. Sanitize Feature Names to Remove Special Characters\n",
    "def sanitize_column_names(columns):\n",
    "    \"\"\"\n",
    "    Replace any special characters in column names with underscores.\n",
    "    Ensures all column names are strings before applying regex.\n",
    "    \"\"\"\n",
    "    return [re.sub(r'[^A-Za-z0-9_]', '_', str(col)) for col in columns]\n",
    "\n",
    "# Apply sanitization\n",
    "X_train.columns = sanitize_column_names(X_train.columns)\n",
    "X_test.columns = sanitize_column_names(X_test.columns)\n",
    "\n",
    "# 7. Model Training and Evaluation\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(true, predicted, model_name):\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    return rmse\n",
    "\n",
    "# Function to plot predictions\n",
    "def plot_predictions(true, predicted, model_name):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    # Check if 'true' is a pandas Series/DataFrame or a numpy array\n",
    "    if isinstance(true, (pd.Series, pd.DataFrame)):\n",
    "        true_values = true.values\n",
    "    elif isinstance(true, np.ndarray):\n",
    "        true_values = true\n",
    "    else:\n",
    "        true_values = np.array(true)\n",
    "    \n",
    "    if isinstance(predicted, (pd.Series, pd.DataFrame)):\n",
    "        predicted_values = predicted.values\n",
    "    elif isinstance(predicted, np.ndarray):\n",
    "        predicted_values = predicted\n",
    "    else:\n",
    "        predicted_values = np.array(predicted)\n",
    "    \n",
    "    plt.plot(range(len(true_values)), true_values, label='Actual', color='blue')\n",
    "    plt.plot(range(len(predicted_values)), predicted_values, label='Predicted', color='red', alpha=0.7)\n",
    "    plt.title(f'Actual vs Predicted Closing Prices ({model_name})')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Closing Price (Scaled)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "### **Model 1: LightGBM Regressor**\n",
    "\n",
    "# Initialize the model\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    print(\"LightGBM training completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LightGBM training: {e}\")\n",
    "\n",
    "# Predict\n",
    "try:\n",
    "    y_pred_lgbm = lgbm.predict(X_test)\n",
    "    print(\"LightGBM prediction completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LightGBM prediction: {e}\")\n",
    "\n",
    "# Evaluate\n",
    "if 'y_pred_lgbm' in locals():\n",
    "    rmse_lgbm = evaluate_model(y_test, y_pred_lgbm, 'LightGBM Regressor')\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_predictions(y_test, y_pred_lgbm, 'LightGBM Regressor')\n",
    "else:\n",
    "    rmse_lgbm = None\n",
    "    print(\"Skipping evaluation and plotting for LightGBM due to previous errors.\")\n",
    "\n",
    "### **Model 2: ARIMA**\n",
    "\n",
    "# For ARIMA, we'll use the original data without scaling\n",
    "# Ensure stationarity\n",
    "result = adfuller(train_data['y'])\n",
    "print('\\nAugmented Dickey-Fuller Test:')\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "\n",
    "if result[1] > 0.05:\n",
    "    # Differencing to make the series stationary\n",
    "    train_data['y_diff'] = train_data['y'].diff().dropna()\n",
    "    print(\"Applied differencing to make the series stationary.\")\n",
    "else:\n",
    "    train_data['y_diff'] = train_data['y']\n",
    "    print(\"Series is stationary. No differencing applied.\")\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_order = (5, 1, 0)  # This can be adjusted or determined using AIC/BIC criteria\n",
    "arima_model = ARIMA(train_data['y'], order=arima_order)\n",
    "try:\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    print(\"ARIMA model fitted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during ARIMA model fitting: {e}\")\n",
    "\n",
    "# Forecast\n",
    "forecast_steps = len(test_data)\n",
    "try:\n",
    "    arima_forecast = arima_model_fit.forecast(steps=forecast_steps)\n",
    "    print(\"ARIMA forecasting completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during ARIMA forecasting: {e}\")\n",
    "\n",
    "# Evaluate\n",
    "try:\n",
    "    rmse_arima = evaluate_model(test_data['y'], arima_forecast, 'ARIMA')\n",
    "except Exception as e:\n",
    "    print(f\"Error during ARIMA evaluation: {e}\")\n",
    "    rmse_arima = None\n",
    "\n",
    "# Plot predictions\n",
    "if rmse_arima is not None:\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_data['Date'].reset_index(drop=True), test_data['y'].reset_index(drop=True), label='Actual', color='blue')\n",
    "    plt.plot(test_data['Date'].reset_index(drop=True), arima_forecast.values, label='Predicted', color='red', alpha=0.7)\n",
    "    plt.title('Actual vs Predicted Closing Prices (ARIMA)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Closing Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping ARIMA plotting due to previous errors.\")\n",
    "\n",
    "### **Model 3: LSTM Neural Network**\n",
    "\n",
    "# Prepare data for LSTM\n",
    "def create_lstm_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.float32)\n",
    "\n",
    "time_steps = 10\n",
    "\n",
    "# Combine train and test data for LSTM scaling\n",
    "combined_data = pd.concat([train_data_scaled, test_data_scaled], axis=0)\n",
    "X_lstm = combined_data.drop(columns=['Date', 'y'])\n",
    "y_lstm = combined_data['y']\n",
    "\n",
    "# Create datasets\n",
    "X_lstm, y_lstm = create_lstm_dataset(X_lstm, y_lstm, time_steps)\n",
    "\n",
    "# Split back into train and test\n",
    "X_train_lstm = X_lstm[:split_index - time_steps]\n",
    "y_train_lstm = y_lstm[:split_index - time_steps]\n",
    "\n",
    "X_test_lstm = X_lstm[split_index - time_steps:]\n",
    "y_test_lstm = y_lstm[split_index - time_steps:]\n",
    "\n",
    "# Build LSTM Model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(32, return_sequences=False))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    history = lstm_model.fit(X_train_lstm, y_train_lstm, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "    print(\"LSTM training completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LSTM training: {e}\")\n",
    "\n",
    "# Predict\n",
    "try:\n",
    "    y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
    "    print(\"LSTM prediction completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LSTM prediction: {e}\")\n",
    "\n",
    "# Evaluate\n",
    "if 'y_pred_lstm' in locals():\n",
    "    rmse_lstm = evaluate_model(y_test_lstm, y_pred_lstm.flatten(), 'LSTM')\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_predictions(y_test_lstm, y_pred_lstm.flatten(), 'LSTM')\n",
    "else:\n",
    "    rmse_lstm = None\n",
    "    print(\"Skipping evaluation and plotting for LSTM due to previous errors.\")\n",
    "\n",
    "# 8. Model Comparison\n",
    "\n",
    "print(\"\\nModel RMSE Comparison:\")\n",
    "if rmse_lgbm is not None:\n",
    "    print(f\"LightGBM Regressor RMSE: {rmse_lgbm:.4f}\")\n",
    "if rmse_arima is not None:\n",
    "    print(f\"ARIMA RMSE: {rmse_arima:.4f}\")\n",
    "if rmse_lstm is not None:\n",
    "    print(f\"LSTM RMSE: {rmse_lstm:.4f}\")\n",
    "\n",
    "# 9. Final Model Selection and Prediction\n",
    "\n",
    "# Assuming LightGBM performed the best based on RMSE\n",
    "if rmse_lgbm is not None and (rmse_arima is None or rmse_lgbm <= rmse_arima) and (rmse_lstm is None or rmse_lgbm <= rmse_lstm):\n",
    "    print(\"\\nSelecting LightGBM Regressor as the final model.\")\n",
    "    # Retrain on entire dataset\n",
    "    full_data = data.copy()\n",
    "    full_data[features_to_scale] = scaler.fit_transform(full_data[features_to_scale])\n",
    "    \n",
    "    X_full = full_data.drop(columns=['Date', 'y'])\n",
    "    y_full = full_data['y']\n",
    "    \n",
    "    # Sanitize column names in the full dataset\n",
    "    X_full.columns = sanitize_column_names(X_full.columns)\n",
    "    \n",
    "    lgbm_final = LGBMRegressor(random_state=42)\n",
    "    try:\n",
    "        lgbm_final.fit(X_full, y_full)\n",
    "        print(\"Final LightGBM training completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during final LightGBM training: {e}\")\n",
    "    \n",
    "    # Future Prediction (Next Day Closing Price)\n",
    "    # Prepare features for the next day\n",
    "    last_row = full_data.iloc[-1:].copy()\n",
    "    \n",
    "    # Increment the date by one day\n",
    "    next_date = last_row['Date'] + timedelta(days=1)\n",
    "    last_row['Date'] = next_date\n",
    "    \n",
    "    # Update time-based features\n",
    "    last_row['DayOfWeek'] = last_row['Date'].dt.dayofweek\n",
    "    last_row['WeekOfYear'] = last_row['Date'].dt.isocalendar().week\n",
    "    last_row['Month'] = last_row['Date'].dt.month\n",
    "    last_row['Quarter'] = last_row['Date'].dt.quarter\n",
    "    last_row['Year'] = last_row['Date'].dt.year\n",
    "    \n",
    "    # Update lag features\n",
    "    for lag in range(1, 8):\n",
    "        if lag <= len(full_data):\n",
    "            last_row[f'Lag_{lag}'] = full_data['y'].shift(lag).iloc[-1]\n",
    "        else:\n",
    "            last_row[f'Lag_{lag}'] = 0  # Assign 0 or some default value if lag exceeds data length\n",
    "    \n",
    "    # Update rolling features\n",
    "    last_row['RollingMean_7'] = full_data['y'].rolling(window=7).mean().iloc[-1]\n",
    "    last_row['RollingStd_7'] = full_data['y'].rolling(window=7).std().iloc[-1]\n",
    "    last_row['RollingMean_14'] = full_data['y'].rolling(window=14).mean().iloc[-1]\n",
    "    last_row['RollingStd_14'] = full_data['y'].rolling(window=14).std().iloc[-1]\n",
    "    \n",
    "    # Drop NaN values\n",
    "    last_row.dropna(inplace=True)\n",
    "    \n",
    "    # Scale features\n",
    "    last_row[features_to_scale] = scaler.transform(last_row[features_to_scale])\n",
    "    \n",
    "    # Sanitize column names\n",
    "    last_row = last_row.rename(columns=lambda x: re.sub(r'[^A-Za-z0-9_]', '_', x))\n",
    "    \n",
    "    # Prepare input features\n",
    "    X_next = last_row.drop(columns=['Date', 'y'])\n",
    "    \n",
    "    # Predict the next day's closing price\n",
    "    try:\n",
    "        next_day_prediction_scaled = lgbm_final.predict(X_next)\n",
    "        print(\"Next day prediction completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LightGBM prediction for next day: {e}\")\n",
    "    \n",
    "    # Inverse transform the prediction\n",
    "    # Since only 'y' was scaled, we need to inverse transform only that component\n",
    "    # Create an array with zeros for other features\n",
    "    inverse_transform_input = np.zeros((1, len(features_to_scale)))\n",
    "    inverse_transform_input[0, -1] = next_day_prediction_scaled[0]\n",
    "    next_day_prediction = scaler.inverse_transform(inverse_transform_input)[0, -1]\n",
    "    \n",
    "    print(f\"\\nPredicted Next Day Closing Price: {next_day_prediction:.2f}\")\n",
    "else:\n",
    "    print(\"\\nLightGBM Regressor did not perform the best. Skipping final model selection and prediction.\")\n",
    "\n",
    "# 10. Plotting the Actual vs Predicted Prices for Test Data\n",
    "\n",
    "# Inverse transform the scaled prices\n",
    "y_test_actual = scaler.inverse_transform(\n",
    "    np.concatenate((np.zeros((len(y_test), len(features_to_scale)-1)), y_test.values.reshape(-1, 1)), axis=1)\n",
    ")[:, -1]\n",
    "y_pred_actual = scaler.inverse_transform(\n",
    "    np.concatenate((np.zeros((len(y_pred_lgbm), len(features_to_scale)-1)), y_pred_lgbm.reshape(-1, 1)), axis=1)\n",
    ")[:, -1]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_data['Date'].reset_index(drop=True), y_test_actual, label='Actual', color='blue')\n",
    "plt.plot(test_data['Date'].reset_index(drop=True), y_pred_actual, label='Predicted', color='red', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Closing Prices (LightGBM)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 11. Feature Importance Visualization\n",
    "\n",
    "if rmse_lgbm is not None:\n",
    "    importances = lgbm.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    # Create a DataFrame for visualization\n",
    "    feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(feature_importances['Feature'], feature_importances['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances from LightGBM Regressor')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# Note: Uncomment the following line if running in a fresh environment\n",
    "# !pip install yfinance pandas numpy scikit-learn matplotlib lightgbm xgboost catboost statsmodels tensorflow\n",
    "\n",
    "# Import libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# 1. Data Acquisition\n",
    "stock_symbol = 'SOXL'\n",
    "start_date = '2010-03-11'\n",
    "end_date = '2024-11-22'\n",
    "\n",
    "# Download stock data\n",
    "data = yf.download(stock_symbol, start=start_date, end=end_date)\n",
    "\n",
    "# Check if data was downloaded successfully\n",
    "if data.empty:\n",
    "    raise ValueError(f\"No data found for stock symbol {stock_symbol} between {start_date} and {end_date}.\")\n",
    "\n",
    "# 2. Data Preprocessing\n",
    "# We'll use the 'Close' price for prediction\n",
    "data = data[['Close']].copy()\n",
    "data.rename(columns={'Close': 'y'}, inplace=True)\n",
    "\n",
    "# Reset index to have 'Date' as a column\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "# Ensure data is sorted by date\n",
    "data.sort_values('Date', inplace=True)\n",
    "\n",
    "# Feature Engineering: Create additional time-based features\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data['DayOfWeek'] = data['Date'].dt.dayofweek\n",
    "data['WeekOfYear'] = data['Date'].dt.isocalendar().week\n",
    "data['Month'] = data['Date'].dt.month\n",
    "data['Quarter'] = data['Date'].dt.quarter\n",
    "data['Year'] = data['Date'].dt.year\n",
    "\n",
    "# Lag features (previous day's closing price)\n",
    "for lag in range(1, 8):  # Using 7 days lag\n",
    "    data[f'Lag_{lag}'] = data['y'].shift(lag)\n",
    "\n",
    "# Rolling window features\n",
    "data['RollingMean_7'] = data['y'].rolling(window=7).mean()\n",
    "data['RollingStd_7'] = data['y'].rolling(window=7).std()\n",
    "\n",
    "data['RollingMean_14'] = data['y'].rolling(window=14).mean()\n",
    "data['RollingStd_14'] = data['y'].rolling(window=14).std()\n",
    "\n",
    "# Drop rows with NaN values resulting from lag and rolling calculations\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# 3. Train-Test Split\n",
    "# We'll use the last 20% of the data as the test set\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(data) * split_ratio)\n",
    "\n",
    "train_data = data[:split_index]\n",
    "test_data = data[split_index:]\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# 4. Feature Scaling\n",
    "# Define two separate feature lists\n",
    "features_to_scale_all = ['y'] + [f'Lag_{lag}' for lag in range(1, 8)] + \\\n",
    "                        ['RollingMean_7', 'RollingStd_7', 'RollingMean_14', 'RollingStd_14']\n",
    "\n",
    "features_to_scale_input = [f for f in features_to_scale_all if f != 'y']\n",
    "\n",
    "# Initialize scalers\n",
    "scaler_all = StandardScaler()\n",
    "scaler_input = StandardScaler()\n",
    "\n",
    "# Scale all features including 'y' for training and testing datasets\n",
    "train_data_scaled = train_data.copy()\n",
    "test_data_scaled = test_data.copy()\n",
    "\n",
    "train_data_scaled[features_to_scale_all] = scaler_all.fit_transform(train_data[features_to_scale_all])\n",
    "test_data_scaled[features_to_scale_all] = scaler_all.transform(test_data[features_to_scale_all])\n",
    "\n",
    "# Scale input features separately if needed (optional)\n",
    "# train_data_scaled[features_to_scale_input] = scaler_input.fit_transform(train_data[features_to_scale_input])\n",
    "# test_data_scaled[features_to_scale_input] = scaler_input.transform(test_data[features_to_scale_input])\n",
    "\n",
    "# 5. Prepare Training and Testing Data\n",
    "X_train = train_data_scaled.drop(columns=['Date', 'y'])\n",
    "y_train = train_data_scaled['y']\n",
    "\n",
    "X_test = test_data_scaled.drop(columns=['Date', 'y'])\n",
    "y_test = test_data_scaled['y']\n",
    "\n",
    "# 6. Sanitize Feature Names to Remove Special Characters\n",
    "def sanitize_column_names(columns):\n",
    "    \"\"\"\n",
    "    Replace any special characters in column names with underscores.\n",
    "    Ensures all column names are strings before applying regex.\n",
    "    \"\"\"\n",
    "    return [re.sub(r'[^A-Za-z0-9_]', '_', str(col)) for col in columns]\n",
    "\n",
    "# Apply sanitization\n",
    "X_train.columns = sanitize_column_names(X_train.columns)\n",
    "X_test.columns = sanitize_column_names(X_test.columns)\n",
    "\n",
    "# 7. Model Training and Evaluation\n",
    "\n",
    "# Function to evaluate model performance\n",
    "def evaluate_model(true, predicted, model_name):\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    return rmse\n",
    "\n",
    "# Function to plot predictions\n",
    "def plot_predictions(true, predicted, model_name):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    # Check if 'true' is a pandas Series/DataFrame or a numpy array\n",
    "    if isinstance(true, (pd.Series, pd.DataFrame)):\n",
    "        true_values = true.values\n",
    "    elif isinstance(true, np.ndarray):\n",
    "        true_values = true\n",
    "    else:\n",
    "        true_values = np.array(true)\n",
    "    \n",
    "    if isinstance(predicted, (pd.Series, pd.DataFrame)):\n",
    "        predicted_values = predicted.values\n",
    "    elif isinstance(predicted, np.ndarray):\n",
    "        predicted_values = predicted\n",
    "    else:\n",
    "        predicted_values = np.array(predicted)\n",
    "    \n",
    "    plt.plot(range(len(true_values)), true_values, label='Actual', color='blue')\n",
    "    plt.plot(range(len(predicted_values)), predicted_values, label='Predicted', color='red', alpha=0.7)\n",
    "    plt.title(f'Actual vs Predicted Closing Prices ({model_name})')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Closing Price (Scaled)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "### **Model 1: LightGBM Regressor**\n",
    "\n",
    "# Initialize the model\n",
    "lgbm = LGBMRegressor(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    lgbm.fit(X_train, y_train)\n",
    "    print(\"LightGBM training completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LightGBM training: {e}\")\n",
    "\n",
    "# Predict\n",
    "try:\n",
    "    y_pred_lgbm = lgbm.predict(X_test)\n",
    "    print(\"LightGBM prediction completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LightGBM prediction: {e}\")\n",
    "\n",
    "# Evaluate\n",
    "if 'y_pred_lgbm' in locals():\n",
    "    rmse_lgbm = evaluate_model(y_test, y_pred_lgbm, 'LightGBM Regressor')\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_predictions(y_test, y_pred_lgbm, 'LightGBM Regressor')\n",
    "else:\n",
    "    rmse_lgbm = None\n",
    "    print(\"Skipping evaluation and plotting for LightGBM due to previous errors.\")\n",
    "\n",
    "### **Model 2: ARIMA**\n",
    "\n",
    "# For ARIMA, we'll use the original data without scaling\n",
    "# Ensure stationarity\n",
    "result = adfuller(train_data['y'])\n",
    "print('\\nAugmented Dickey-Fuller Test:')\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "\n",
    "if result[1] > 0.05:\n",
    "    # Differencing to make the series stationary\n",
    "    train_data['y_diff'] = train_data['y'].diff().dropna()\n",
    "    print(\"Applied differencing to make the series stationary.\")\n",
    "else:\n",
    "    train_data['y_diff'] = train_data['y']\n",
    "    print(\"Series is stationary. No differencing applied.\")\n",
    "\n",
    "# Fit ARIMA model\n",
    "arima_order = (5, 1, 0)  # This can be adjusted or determined using AIC/BIC criteria\n",
    "arima_model = ARIMA(train_data['y'], order=arima_order)\n",
    "try:\n",
    "    arima_model_fit = arima_model.fit()\n",
    "    print(\"ARIMA model fitted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during ARIMA model fitting: {e}\")\n",
    "\n",
    "# Forecast\n",
    "forecast_steps = len(test_data)\n",
    "try:\n",
    "    arima_forecast = arima_model_fit.forecast(steps=forecast_steps)\n",
    "    print(\"ARIMA forecasting completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during ARIMA forecasting: {e}\")\n",
    "\n",
    "# Evaluate\n",
    "try:\n",
    "    rmse_arima = evaluate_model(test_data['y'], arima_forecast, 'ARIMA')\n",
    "except Exception as e:\n",
    "    print(f\"Error during ARIMA evaluation: {e}\")\n",
    "    rmse_arima = None\n",
    "\n",
    "# Plot predictions\n",
    "if rmse_arima is not None:\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(test_data['Date'].reset_index(drop=True), test_data['y'].reset_index(drop=True), label='Actual', color='blue')\n",
    "    plt.plot(test_data['Date'].reset_index(drop=True), arima_forecast.values, label='Predicted', color='red', alpha=0.7)\n",
    "    plt.title('Actual vs Predicted Closing Prices (ARIMA)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Closing Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping ARIMA plotting due to previous errors.\")\n",
    "\n",
    "### **Model 3: LSTM Neural Network**\n",
    "\n",
    "# Prepare data for LSTM\n",
    "def create_lstm_dataset(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs, dtype=np.float32), np.array(ys, dtype=np.float32)\n",
    "\n",
    "time_steps = 10\n",
    "\n",
    "# Combine train and test data for LSTM scaling\n",
    "combined_data = pd.concat([train_data_scaled, test_data_scaled], axis=0)\n",
    "X_lstm = combined_data.drop(columns=['Date', 'y'])\n",
    "y_lstm = combined_data['y']\n",
    "\n",
    "# Create datasets\n",
    "X_lstm, y_lstm = create_lstm_dataset(X_lstm, y_lstm, time_steps)\n",
    "\n",
    "# Split back into train and test\n",
    "X_train_lstm = X_lstm[:split_index - time_steps]\n",
    "y_train_lstm = y_lstm[:split_index - time_steps]\n",
    "\n",
    "X_test_lstm = X_lstm[split_index - time_steps:]\n",
    "y_test_lstm = y_lstm[split_index - time_steps:]\n",
    "\n",
    "# Build LSTM Model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(64, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2]), return_sequences=True))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(LSTM(32, return_sequences=False))\n",
    "lstm_model.add(Dropout(0.2))\n",
    "lstm_model.add(Dense(1))\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "try:\n",
    "    history = lstm_model.fit(X_train_lstm, y_train_lstm, epochs=20, batch_size=32, validation_split=0.1, verbose=1)\n",
    "    print(\"LSTM training completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LSTM training: {e}\")\n",
    "\n",
    "# Predict\n",
    "try:\n",
    "    y_pred_lstm = lstm_model.predict(X_test_lstm)\n",
    "    print(\"LSTM prediction completed successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during LSTM prediction: {e}\")\n",
    "\n",
    "# Evaluate\n",
    "if 'y_pred_lstm' in locals():\n",
    "    rmse_lstm = evaluate_model(y_test_lstm, y_pred_lstm.flatten(), 'LSTM')\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_predictions(y_test_lstm, y_pred_lstm.flatten(), 'LSTM')\n",
    "else:\n",
    "    rmse_lstm = None\n",
    "    print(\"Skipping evaluation and plotting for LSTM due to previous errors.\")\n",
    "\n",
    "# 8. Model Comparison\n",
    "\n",
    "print(\"\\nModel RMSE Comparison:\")\n",
    "if rmse_lgbm is not None:\n",
    "    print(f\"LightGBM Regressor RMSE: {rmse_lgbm:.4f}\")\n",
    "if rmse_arima is not None:\n",
    "    print(f\"ARIMA RMSE: {rmse_arima:.4f}\")\n",
    "if rmse_lstm is not None:\n",
    "    print(f\"LSTM RMSE: {rmse_lstm:.4f}\")\n",
    "\n",
    "# 9. Final Model Selection and Prediction\n",
    "\n",
    "# Assuming LightGBM performed the best based on RMSE\n",
    "if rmse_lgbm is not None and (rmse_arima is None or rmse_lgbm <= rmse_arima) and (rmse_lstm is None or rmse_lgbm <= rmse_lstm):\n",
    "    print(\"\\nSelecting LightGBM Regressor as the final model.\")\n",
    "    # Retrain on entire dataset\n",
    "    full_data = data.copy()\n",
    "    full_data[features_to_scale_all] = scaler_all.fit_transform(full_data[features_to_scale_all])\n",
    "    \n",
    "    X_full = full_data.drop(columns=['Date', 'y'])\n",
    "    y_full = full_data['y']\n",
    "    \n",
    "    # Sanitize column names in the full dataset\n",
    "    X_full.columns = sanitize_column_names(X_full.columns)\n",
    "    \n",
    "    lgbm_final = LGBMRegressor(random_state=42)\n",
    "    try:\n",
    "        lgbm_final.fit(X_full, y_full)\n",
    "        print(\"Final LightGBM training completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during final LightGBM training: {e}\")\n",
    "    \n",
    "    # Future Prediction (Next Day Closing Price)\n",
    "    # Prepare features for the next day\n",
    "    last_row = full_data.iloc[-1:].copy()\n",
    "    \n",
    "    # Increment the date by one day\n",
    "    next_date = last_row['Date'] + timedelta(days=1)\n",
    "    last_row['Date'] = next_date\n",
    "    \n",
    "    # Update time-based features\n",
    "    last_row['DayOfWeek'] = last_row['Date'].dt.dayofweek\n",
    "    last_row['WeekOfYear'] = last_row['Date'].dt.isocalendar().week\n",
    "    last_row['Month'] = last_row['Date'].dt.month\n",
    "    last_row['Quarter'] = last_row['Date'].dt.quarter\n",
    "    last_row['Year'] = last_row['Date'].dt.year\n",
    "    \n",
    "    # Update lag features\n",
    "    for lag in range(1, 8):\n",
    "        if lag <= len(full_data):\n",
    "            last_row[f'Lag_{lag}'] = full_data['y'].shift(lag).iloc[-1]\n",
    "        else:\n",
    "            last_row[f'Lag_{lag}'] = 0  # Assign 0 or some default value if lag exceeds data length\n",
    "    \n",
    "    # Update rolling features\n",
    "    last_row['RollingMean_7'] = full_data['y'].rolling(window=7).mean().iloc[-1]\n",
    "    last_row['RollingStd_7'] = full_data['y'].rolling(window=7).std().iloc[-1]\n",
    "    last_row['RollingMean_14'] = full_data['y'].rolling(window=14).mean().iloc[-1]\n",
    "    last_row['RollingStd_14'] = full_data['y'].rolling(window=14).std().iloc[-1]\n",
    "    \n",
    "    # Drop NaN values\n",
    "    last_row.dropna(inplace=True)\n",
    "    \n",
    "    # Define input features excluding 'y'\n",
    "    input_features = [f for f in features_to_scale_all if f != 'y']\n",
    "    \n",
    "    # Check if all input features are present\n",
    "    missing_features = set(input_features) - set(last_row.columns)\n",
    "    if missing_features:\n",
    "        print(f\"Missing features in last_row: {missing_features}\")\n",
    "        # Handle missing features if necessary\n",
    "        # For now, we'll assign 0 to missing features\n",
    "        for feature in missing_features:\n",
    "            last_row[feature] = 0\n",
    "    \n",
    "    # Scale input features\n",
    "    try:\n",
    "        last_row[input_features] = scaler_all.transform(last_row[input_features])\n",
    "        print(\"Scaling of input features for prediction completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during scaling of input features: {e}\")\n",
    "        # Exit prediction process if scaling fails\n",
    "        next_day_prediction = None\n",
    "    \n",
    "    # Sanitize column names\n",
    "    last_row = last_row.rename(columns=lambda x: re.sub(r'[^A-Za-z0-9_]', '_', x))\n",
    "    \n",
    "    # Prepare input features\n",
    "    X_next = last_row.drop(columns=['Date', 'y'], errors='ignore')  # 'y' might not be present\n",
    "    \n",
    "    # Ensure that all input features are present\n",
    "    for feature in X_full.columns:\n",
    "        if feature not in X_next.columns:\n",
    "            X_next[feature] = 0  # Assign default value\n",
    "    \n",
    "    # Reorder columns to match the training data\n",
    "    X_next = X_next[X_full.columns]\n",
    "    \n",
    "    # Predict the next day's closing price\n",
    "    try:\n",
    "        next_day_prediction_scaled = lgbm_final.predict(X_next)\n",
    "        print(\"Next day prediction completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LightGBM prediction for next day: {e}\")\n",
    "        next_day_prediction_scaled = None\n",
    "    \n",
    "    if next_day_prediction_scaled is not None:\n",
    "        # Inverse transform the prediction\n",
    "        # Since 'y' is the last feature in scaler_all, we can reconstruct the array\n",
    "        inverse_transform_input = np.zeros((1, len(features_to_scale_all)))\n",
    "        inverse_transform_input[0, features_to_scale_all.index('y')] = next_day_prediction_scaled[0]\n",
    "        next_day_prediction = scaler_all.inverse_transform(inverse_transform_input)[0, features_to_scale_all.index('y')]\n",
    "        \n",
    "        print(f\"\\nPredicted Next Day Closing Price: {next_day_prediction:.2f}\")\n",
    "    else:\n",
    "        print(\"\\nNext day prediction could not be made due to previous errors.\")\n",
    "else:\n",
    "    print(\"\\nLightGBM Regressor did not perform the best. Skipping final model selection and prediction.\")\n",
    "\n",
    "# 10. Plotting the Actual vs Predicted Prices for Test Data\n",
    "\n",
    "# Inverse transform the scaled prices\n",
    "y_test_actual = scaler_all.inverse_transform(\n",
    "    np.concatenate((np.zeros((len(y_test), len(features_to_scale_all)-1)), y_test.values.reshape(-1, 1)), axis=1)\n",
    ")[:, -1]\n",
    "y_pred_actual = scaler_all.inverse_transform(\n",
    "    np.concatenate((np.zeros((len(y_pred_lgbm), len(features_to_scale_all)-1)), y_pred_lgbm.reshape(-1, 1)), axis=1)\n",
    ")[:, -1]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(test_data['Date'].reset_index(drop=True), y_test_actual, label='Actual', color='blue')\n",
    "plt.plot(test_data['Date'].reset_index(drop=True), y_pred_actual, label='Predicted', color='red', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Closing Prices (LightGBM)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 11. Feature Importance Visualization\n",
    "\n",
    "if rmse_lgbm is not None:\n",
    "    importances = lgbm.feature_importances_\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    # Create a DataFrame for visualization\n",
    "    feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(feature_importances['Feature'], feature_importances['Importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importances from LightGBM Regressor')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d9fa5_row25_col1 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d9fa5\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d9fa5_level0_col0\" class=\"col_heading level0 col0\" >Description</th>\n",
       "      <th id=\"T_d9fa5_level0_col1\" class=\"col_heading level0 col1\" >Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d9fa5_row0_col0\" class=\"data row0 col0\" >session_id</td>\n",
       "      <td id=\"T_d9fa5_row0_col1\" class=\"data row0 col1\" >42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d9fa5_row1_col0\" class=\"data row1 col0\" >Target</td>\n",
       "      <td id=\"T_d9fa5_row1_col1\" class=\"data row1 col1\" >Open</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d9fa5_row2_col0\" class=\"data row2 col0\" >Approach</td>\n",
       "      <td id=\"T_d9fa5_row2_col1\" class=\"data row2 col1\" >Univariate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d9fa5_row3_col0\" class=\"data row3 col0\" >Exogenous Variables</td>\n",
       "      <td id=\"T_d9fa5_row3_col1\" class=\"data row3 col1\" >Not Present</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d9fa5_row4_col0\" class=\"data row4 col0\" >Original data shape</td>\n",
       "      <td id=\"T_d9fa5_row4_col1\" class=\"data row4 col1\" >(3703, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d9fa5_row5_col0\" class=\"data row5 col0\" >Transformed data shape</td>\n",
       "      <td id=\"T_d9fa5_row5_col1\" class=\"data row5 col1\" >(3703, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_d9fa5_row6_col0\" class=\"data row6 col0\" >Transformed train set shape</td>\n",
       "      <td id=\"T_d9fa5_row6_col1\" class=\"data row6 col1\" >(3696, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_d9fa5_row7_col0\" class=\"data row7 col0\" >Transformed test set shape</td>\n",
       "      <td id=\"T_d9fa5_row7_col1\" class=\"data row7 col1\" >(7, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_d9fa5_row8_col0\" class=\"data row8 col0\" >Rows with missing values</td>\n",
       "      <td id=\"T_d9fa5_row8_col1\" class=\"data row8 col1\" >0.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_d9fa5_row9_col0\" class=\"data row9 col0\" >Fold Generator</td>\n",
       "      <td id=\"T_d9fa5_row9_col1\" class=\"data row9 col1\" >ExpandingWindowSplitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_d9fa5_row10_col0\" class=\"data row10 col0\" >Fold Number</td>\n",
       "      <td id=\"T_d9fa5_row10_col1\" class=\"data row10 col1\" >3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_d9fa5_row11_col0\" class=\"data row11 col0\" >Enforce Prediction Interval</td>\n",
       "      <td id=\"T_d9fa5_row11_col1\" class=\"data row11 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_d9fa5_row12_col0\" class=\"data row12 col0\" >Splits used for hyperparameters</td>\n",
       "      <td id=\"T_d9fa5_row12_col1\" class=\"data row12 col1\" >all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_d9fa5_row13_col0\" class=\"data row13 col0\" >User Defined Seasonal Period(s)</td>\n",
       "      <td id=\"T_d9fa5_row13_col1\" class=\"data row13 col1\" >None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_d9fa5_row14_col0\" class=\"data row14 col0\" >Ignore Seasonality Test</td>\n",
       "      <td id=\"T_d9fa5_row14_col1\" class=\"data row14 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_d9fa5_row15_col0\" class=\"data row15 col0\" >Seasonality Detection Algo</td>\n",
       "      <td id=\"T_d9fa5_row15_col1\" class=\"data row15 col1\" >auto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_d9fa5_row16_col0\" class=\"data row16 col0\" >Max Period to Consider</td>\n",
       "      <td id=\"T_d9fa5_row16_col1\" class=\"data row16 col1\" >60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_d9fa5_row17_col0\" class=\"data row17 col0\" >Seasonal Period(s) Tested</td>\n",
       "      <td id=\"T_d9fa5_row17_col1\" class=\"data row17 col1\" >[18, 49, 4, 6, 32, 59, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_d9fa5_row18_col0\" class=\"data row18 col0\" >Significant Seasonal Period(s)</td>\n",
       "      <td id=\"T_d9fa5_row18_col1\" class=\"data row18 col1\" >[18, 49, 4, 6, 32, 59, 30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_d9fa5_row19_col0\" class=\"data row19 col0\" >Significant Seasonal Period(s) without Harmonics</td>\n",
       "      <td id=\"T_d9fa5_row19_col1\" class=\"data row19 col1\" >[18, 49, 32, 30, 59]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_d9fa5_row20_col0\" class=\"data row20 col0\" >Remove Harmonics</td>\n",
       "      <td id=\"T_d9fa5_row20_col1\" class=\"data row20 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_d9fa5_row21_col0\" class=\"data row21 col0\" >Harmonics Order Method</td>\n",
       "      <td id=\"T_d9fa5_row21_col1\" class=\"data row21 col1\" >harmonic_max</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_d9fa5_row22_col0\" class=\"data row22 col0\" >Num Seasonalities to Use</td>\n",
       "      <td id=\"T_d9fa5_row22_col1\" class=\"data row22 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_d9fa5_row23_col0\" class=\"data row23 col0\" >All Seasonalities to Use</td>\n",
       "      <td id=\"T_d9fa5_row23_col1\" class=\"data row23 col1\" >[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_d9fa5_row24_col0\" class=\"data row24 col0\" >Primary Seasonality</td>\n",
       "      <td id=\"T_d9fa5_row24_col1\" class=\"data row24 col1\" >18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_d9fa5_row25_col0\" class=\"data row25 col0\" >Seasonality Present</td>\n",
       "      <td id=\"T_d9fa5_row25_col1\" class=\"data row25 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_d9fa5_row26_col0\" class=\"data row26 col0\" >Seasonality Type</td>\n",
       "      <td id=\"T_d9fa5_row26_col1\" class=\"data row26 col1\" >mul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_d9fa5_row27_col0\" class=\"data row27 col0\" >Target Strictly Positive</td>\n",
       "      <td id=\"T_d9fa5_row27_col1\" class=\"data row27 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_d9fa5_row28_col0\" class=\"data row28 col0\" >Target White Noise</td>\n",
       "      <td id=\"T_d9fa5_row28_col1\" class=\"data row28 col1\" >No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_d9fa5_row29_col0\" class=\"data row29 col0\" >Recommended d</td>\n",
       "      <td id=\"T_d9fa5_row29_col1\" class=\"data row29 col1\" >1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_d9fa5_row30_col0\" class=\"data row30 col0\" >Recommended Seasonal D</td>\n",
       "      <td id=\"T_d9fa5_row30_col1\" class=\"data row30 col1\" >0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_d9fa5_row31_col0\" class=\"data row31 col0\" >Preprocess</td>\n",
       "      <td id=\"T_d9fa5_row31_col1\" class=\"data row31 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_d9fa5_row32_col0\" class=\"data row32 col0\" >CPU Jobs</td>\n",
       "      <td id=\"T_d9fa5_row32_col1\" class=\"data row32 col1\" >-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_d9fa5_row33_col0\" class=\"data row33 col0\" >Use GPU</td>\n",
       "      <td id=\"T_d9fa5_row33_col1\" class=\"data row33 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_d9fa5_row34_col0\" class=\"data row34 col0\" >Log Experiment</td>\n",
       "      <td id=\"T_d9fa5_row34_col1\" class=\"data row34 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "      <td id=\"T_d9fa5_row35_col0\" class=\"data row35 col0\" >Experiment Name</td>\n",
       "      <td id=\"T_d9fa5_row35_col1\" class=\"data row35 col1\" >ts-default-name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d9fa5_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_d9fa5_row36_col0\" class=\"data row36 col0\" >USI</td>\n",
       "      <td id=\"T_d9fa5_row36_col1\" class=\"data row36 col1\" >6c12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14b66e140>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>16:32:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Fitting 3 Folds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Auto ARIMA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               \n",
       "                                                               \n",
       "Initiated  . . . . . . . . . . . . . . . . . .         16:32:35\n",
       "Status     . . . . . . . . . . . . . . . . . .  Fitting 3 Folds\n",
       "Estimator  . . . . . . . . . . . . . . . . . .       Auto ARIMA"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_62db4 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_62db4_row0_col0, #T_62db4_row0_col1, #T_62db4_row0_col2, #T_62db4_row0_col3, #T_62db4_row0_col4, #T_62db4_row0_col5, #T_62db4_row0_col6, #T_62db4_row0_col7, #T_62db4_row0_col8, #T_62db4_row1_col0, #T_62db4_row1_col1, #T_62db4_row1_col2, #T_62db4_row1_col3, #T_62db4_row1_col4, #T_62db4_row1_col5, #T_62db4_row1_col6, #T_62db4_row1_col7, #T_62db4_row1_col8, #T_62db4_row2_col0, #T_62db4_row2_col1, #T_62db4_row2_col2, #T_62db4_row2_col3, #T_62db4_row2_col4, #T_62db4_row2_col5, #T_62db4_row2_col6, #T_62db4_row2_col7, #T_62db4_row2_col8, #T_62db4_row3_col0, #T_62db4_row3_col1, #T_62db4_row3_col2, #T_62db4_row3_col3, #T_62db4_row3_col4, #T_62db4_row3_col5, #T_62db4_row3_col6, #T_62db4_row3_col7, #T_62db4_row3_col8, #T_62db4_row4_col0, #T_62db4_row4_col1, #T_62db4_row4_col2, #T_62db4_row4_col3, #T_62db4_row4_col4, #T_62db4_row4_col5, #T_62db4_row4_col6, #T_62db4_row4_col7, #T_62db4_row4_col8 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_62db4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_62db4_level0_col0\" class=\"col_heading level0 col0\" >Model</th>\n",
       "      <th id=\"T_62db4_level0_col1\" class=\"col_heading level0 col1\" >MASE</th>\n",
       "      <th id=\"T_62db4_level0_col2\" class=\"col_heading level0 col2\" >RMSSE</th>\n",
       "      <th id=\"T_62db4_level0_col3\" class=\"col_heading level0 col3\" >MAE</th>\n",
       "      <th id=\"T_62db4_level0_col4\" class=\"col_heading level0 col4\" >RMSE</th>\n",
       "      <th id=\"T_62db4_level0_col5\" class=\"col_heading level0 col5\" >MAPE</th>\n",
       "      <th id=\"T_62db4_level0_col6\" class=\"col_heading level0 col6\" >SMAPE</th>\n",
       "      <th id=\"T_62db4_level0_col7\" class=\"col_heading level0 col7\" >R2</th>\n",
       "      <th id=\"T_62db4_level0_col8\" class=\"col_heading level0 col8\" >TT (Sec)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_62db4_level0_row0\" class=\"row_heading level0 row0\" >naive</th>\n",
       "      <td id=\"T_62db4_row0_col0\" class=\"data row0 col0\" >Naive Forecaster</td>\n",
       "      <td id=\"T_62db4_row0_col1\" class=\"data row0 col1\" >1.5473</td>\n",
       "      <td id=\"T_62db4_row0_col2\" class=\"data row0 col2\" >0.8392</td>\n",
       "      <td id=\"T_62db4_row0_col3\" class=\"data row0 col3\" >3.1800</td>\n",
       "      <td id=\"T_62db4_row0_col4\" class=\"data row0 col4\" >3.5879</td>\n",
       "      <td id=\"T_62db4_row0_col5\" class=\"data row0 col5\" >0.0926</td>\n",
       "      <td id=\"T_62db4_row0_col6\" class=\"data row0 col6\" >0.0899</td>\n",
       "      <td id=\"T_62db4_row0_col7\" class=\"data row0 col7\" >-9.5768</td>\n",
       "      <td id=\"T_62db4_row0_col8\" class=\"data row0 col8\" >1.9900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62db4_level0_row1\" class=\"row_heading level0 row1\" >arima</th>\n",
       "      <td id=\"T_62db4_row1_col0\" class=\"data row1 col0\" >ARIMA</td>\n",
       "      <td id=\"T_62db4_row1_col1\" class=\"data row1 col1\" >1.5543</td>\n",
       "      <td id=\"T_62db4_row1_col2\" class=\"data row1 col2\" >0.8735</td>\n",
       "      <td id=\"T_62db4_row1_col3\" class=\"data row1 col3\" >3.1939</td>\n",
       "      <td id=\"T_62db4_row1_col4\" class=\"data row1 col4\" >3.7348</td>\n",
       "      <td id=\"T_62db4_row1_col5\" class=\"data row1 col5\" >0.0941</td>\n",
       "      <td id=\"T_62db4_row1_col6\" class=\"data row1 col6\" >0.0886</td>\n",
       "      <td id=\"T_62db4_row1_col7\" class=\"data row1 col7\" >-13.1524</td>\n",
       "      <td id=\"T_62db4_row1_col8\" class=\"data row1 col8\" >1.0833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62db4_level0_row2\" class=\"row_heading level0 row2\" >polytrend</th>\n",
       "      <td id=\"T_62db4_row2_col0\" class=\"data row2 col0\" >Polynomial Trend Forecaster</td>\n",
       "      <td id=\"T_62db4_row2_col1\" class=\"data row2 col1\" >1.5551</td>\n",
       "      <td id=\"T_62db4_row2_col2\" class=\"data row2 col2\" >0.8317</td>\n",
       "      <td id=\"T_62db4_row2_col3\" class=\"data row2 col3\" >3.1962</td>\n",
       "      <td id=\"T_62db4_row2_col4\" class=\"data row2 col4\" >3.5559</td>\n",
       "      <td id=\"T_62db4_row2_col5\" class=\"data row2 col5\" >0.0912</td>\n",
       "      <td id=\"T_62db4_row2_col6\" class=\"data row2 col6\" >0.0967</td>\n",
       "      <td id=\"T_62db4_row2_col7\" class=\"data row2 col7\" >-7.4531</td>\n",
       "      <td id=\"T_62db4_row2_col8\" class=\"data row2 col8\" >0.6633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62db4_level0_row3\" class=\"row_heading level0 row3\" >snaive</th>\n",
       "      <td id=\"T_62db4_row3_col0\" class=\"data row3 col0\" >Seasonal Naive Forecaster</td>\n",
       "      <td id=\"T_62db4_row3_col1\" class=\"data row3 col1\" >1.6714</td>\n",
       "      <td id=\"T_62db4_row3_col2\" class=\"data row3 col2\" >0.9222</td>\n",
       "      <td id=\"T_62db4_row3_col3\" class=\"data row3 col3\" >3.4362</td>\n",
       "      <td id=\"T_62db4_row3_col4\" class=\"data row3 col4\" >3.9428</td>\n",
       "      <td id=\"T_62db4_row3_col5\" class=\"data row3 col5\" >0.1037</td>\n",
       "      <td id=\"T_62db4_row3_col6\" class=\"data row3 col6\" >0.0975</td>\n",
       "      <td id=\"T_62db4_row3_col7\" class=\"data row3 col7\" >-5.9632</td>\n",
       "      <td id=\"T_62db4_row3_col8\" class=\"data row3 col8\" >0.0433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_62db4_level0_row4\" class=\"row_heading level0 row4\" >grand_means</th>\n",
       "      <td id=\"T_62db4_row4_col0\" class=\"data row4 col0\" >Grand Means Forecaster</td>\n",
       "      <td id=\"T_62db4_row4_col1\" class=\"data row4 col1\" >10.9461</td>\n",
       "      <td id=\"T_62db4_row4_col2\" class=\"data row4 col2\" >5.2788</td>\n",
       "      <td id=\"T_62db4_row4_col3\" class=\"data row4 col3\" >22.5002</td>\n",
       "      <td id=\"T_62db4_row4_col4\" class=\"data row4 col4\" >22.5685</td>\n",
       "      <td id=\"T_62db4_row4_col5\" class=\"data row4 col5\" >0.6595</td>\n",
       "      <td id=\"T_62db4_row4_col6\" class=\"data row4 col6\" >0.9847</td>\n",
       "      <td id=\"T_62db4_row4_col7\" class=\"data row4 col7\" >-270.4225</td>\n",
       "      <td id=\"T_62db4_row4_col8\" class=\"data row4 col8\" >0.6633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x14c3e9d20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fcf56cd2284e45a3498f4958c0cacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/121 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m exp\u001b[38;5;241m.\u001b[39msetup(data\u001b[38;5;241m=\u001b[39mopen_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m'\u001b[39m], fh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, fold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, session_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# Using a forecast horizon (fh) of 7 days\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Step 5: Train and compare models\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompare_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Step 6: Finalize the model and predict future values\u001b[39;00m\n\u001b[1;32m     24\u001b[0m final_model \u001b[38;5;241m=\u001b[39m exp\u001b[38;5;241m.\u001b[39mfinalize_model(best_model)\n",
      "File \u001b[0;32m~/code/stocks_3_10/.venv/lib/python3.10/site-packages/pycaret/time_series/forecasting/oop.py:2315\u001b[0m, in \u001b[0;36mTSForecastingExperiment.compare_models\u001b[0;34m(self, include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, experiment_custom_tags, engine, verbose, parallel)\u001b[0m\n\u001b[1;32m   2312\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_engine(estimator\u001b[38;5;241m=\u001b[39mestimator, engine\u001b[38;5;241m=\u001b[39meng, severity\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2315\u001b[0m     return_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompare_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2316\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2319\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2321\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_select\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_select\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbudget_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbudget_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mturbo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mturbo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaller_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2331\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2332\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2334\u001b[0m         \u001b[38;5;66;03m# Reset the models back to the default engines\u001b[39;00m\n",
      "File \u001b[0;32m~/code/stocks_3_10/.venv/lib/python3.10/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:794\u001b[0m, in \u001b[0;36m_SupervisedExperiment.compare_models\u001b[0;34m(self, include, exclude, fold, round, cross_validation, sort, n_select, budget_time, turbo, errors, fit_kwargs, groups, experiment_custom_tags, probability_threshold, verbose, parallel, caller_params)\u001b[0m\n\u001b[1;32m    791\u001b[0m results_columns_to_ignore \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruntime\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcutoff\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 794\u001b[0m     model, model_fit_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcreate_model_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m     model_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpull(pop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    797\u001b[0m         np\u001b[38;5;241m.\u001b[39msum(\n\u001b[1;32m    798\u001b[0m             model_results\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    803\u001b[0m     )\n",
      "File \u001b[0;32m~/code/stocks_3_10/.venv/lib/python3.10/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:1533\u001b[0m, in \u001b[0;36m_SupervisedExperiment._create_model\u001b[0;34m(self, estimator, fold, round, cross_validation, predict, fit_kwargs, groups, refit, probability_threshold, experiment_custom_tags, verbose, system, add_to_model_list, X_train_data, y_train_data, metrics, display, model_only, return_train_score, error_score, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model, model_fit_time\n\u001b[1;32m   1531\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m-> 1533\u001b[0m model, model_fit_time, model_results, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_model_with_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_X\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrefit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;66;03m# end runtime\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m runtime_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/code/stocks_3_10/.venv/lib/python3.10/site-packages/pycaret/time_series/forecasting/oop.py:2644\u001b[0m, in \u001b[0;36mTSForecastingExperiment._create_model_with_cv\u001b[0;34m(self, model, data_X, data_y, fit_kwargs, round, cv, metrics, refit, display, **kwargs)\u001b[0m\n\u001b[1;32m   2639\u001b[0m pipeline_with_model \u001b[38;5;241m=\u001b[39m _add_model_to_pipeline(\n\u001b[1;32m   2640\u001b[0m     pipeline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline, model\u001b[38;5;241m=\u001b[39mmodel\n\u001b[1;32m   2641\u001b[0m )\n\u001b[1;32m   2643\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m redirect_output(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger):\n\u001b[0;32m-> 2644\u001b[0m     scores, cutoffs \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_with_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2646\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2652\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2654\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoint_alpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2656\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2657\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_scorer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2660\u001b[0m model_fit_end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   2661\u001b[0m model_fit_time \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(model_fit_end \u001b[38;5;241m-\u001b[39m model_fit_start)\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/code/stocks_3_10/.venv/lib/python3.10/site-packages/pycaret/utils/time_series/forecasting/model_selection.py:306\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(pipeline, y, X, cv, scoring, fit_params, n_jobs, return_train_score, alpha, coverage, error_score, verbose, **additional_scorer_kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m     scoring \u001b[38;5;241m=\u001b[39m _get_metrics_dict(scoring)\n\u001b[1;32m    304\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)\n\u001b[0;32m--> 306\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcoverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m            \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_scorer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_folds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# raise key exceptions\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/stocks_3_10/.venv/lib/python3.10/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/stocks_3_10/.venv/lib/python3.10/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/code/stocks_3_10/.venv/lib/python3.10/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from pycaret.time_series import TSForecastingExperiment\n",
    "\n",
    "# Step 1: Fetch the data using yfinance\n",
    "dat = yf.Ticker(\"SOXL\")\n",
    "df = dat.history(period='max')\n",
    "\n",
    "# Step 2: Extract only the 'Open' column\n",
    "open_data = df[['Open']]\n",
    "\n",
    "# Step 3: Prepare the data for PyCaret\n",
    "open_data.reset_index(inplace=True)\n",
    "open_data.columns = ['Date', 'Open']\n",
    "\n",
    "# Step 4: Initialize PyCaret Time Series Forecasting Experiment\n",
    "exp = TSForecastingExperiment()\n",
    "exp.setup(data=open_data['Open'], fh=7, fold=3, session_id=42)  # Using a forecast horizon (fh) of 7 days\n",
    "\n",
    "# Step 5: Train and compare models\n",
    "best_model = exp.compare_models()\n",
    "\n",
    "# Step 6: Finalize the model and predict future values\n",
    "final_model = exp.finalize_model(best_model)\n",
    "forecast = exp.predict_model(final_model)\n",
    "\n",
    "# Step 7: Plot the forecast\n",
    "exp.plot_model(final_model, plot='forecast')\n",
    "\n",
    "# Optional: Print forecasted values\n",
    "print(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
